# Klaudbiusz

AI-powered Databricks application generator with objective evaluation framework. A thin wrapper around the Claude Agent SDK leveraging the `edda` MCP.

## Overview

Klaudbiusz generates production-ready Databricks applications from natural language prompts and evaluates them using 9 objective, zero-bias metrics. This enables autonomous deployment workflows where AI-generated code can be automatically validated and deployed without human review.

**Current Results:** 90% of generated apps (18/20) are production-ready and deployable.

## Quick Start

### Full Pipeline (Recommended)

Run complete archive â†’ clean â†’ generate â†’ evaluate pipeline:

```bash
cd klaudbiusz
export DATABRICKS_HOST=https://your-workspace.databricks.com
export DATABRICKS_TOKEN=dapi...
export ANTHROPIC_API_KEY=sk-ant-...

# Run BOTH modes for complete comparison (recommended)
./run_all_evals.sh

# Or run individual modes:
./run_vanilla_eval.sh  # Vanilla SDK Mode (Streamlit apps)
./run_mcp_eval.sh      # MCP Mode (TypeScript/tRPC apps)
```

**run_all_evals.sh** runs both modes and produces:
- Side-by-side results in `results_<timestamp>/`
- Comparison summary with timings
- Symlink `results_latest/` for quick access

Each script:
- Archives previous results
- Cleans workspace
- Generates 20 apps
- Runs evaluation
- Records run metadata (timestamps, costs, parameters)

### Manual Generation

Klaudbiusz supports **two modes**:

**MCP Mode (default)** - Uses TypeScript/tRPC stack with MCP tools:
```bash
# Generate a single app
uv run cli/main.py "Create a customer churn analysis dashboard"

# Batch generate from prompts
uv run cli/bulk_run.py
```

**Vanilla SDK Mode** - Pure Claude SDK with embedded context (Streamlit apps):
```bash
# Batch generate
uv run cli/bulk_run.py --enable_mcp=False

# Single app
uv run cli/main.py "Create dashboard" --enable_mcp=False
```

**Cost Comparison:**
- MCP Mode: $0.74/app, ~115 turns
- Vanilla SDK: **$0.27/app, ~33 turns** (63% cheaper, 71% fewer turns)

### View Evaluation Results

**Interactive HTML Viewer** - Select and compare past evaluations:

```bash
# Generate HTML viewer (auto-generated by run scripts)
python3 cli/generate_html_viewer.py

# Open in browser
open evaluation_viewer.html
```

**Features:**
- ğŸ“Š Dropdown to select from all archived evaluations
- ğŸ“ˆ Metrics summary with percentage pass rates
- ğŸ“‹ Detailed app-by-app breakdown
- ğŸ”„ Compares latest runs from both modes

### Manual Evaluation

**Agentic Evaluation** - AI agent with bash tools evaluates apps:

```bash
cd klaudbiusz
export DATABRICKS_HOST=https://your-workspace.databricks.com
export DATABRICKS_TOKEN=dapi...
export ANTHROPIC_API_KEY=sk-ant-...

# Run agentic evaluation
uv run cli/evaluate_all_agent.py

# View in HTML
open evaluation_viewer.html
```

The evaluation agent reads app files, discovers build/test/run commands, executes them, and generates an objective report - no hardcoded logic.

## Evaluation Framework

We use **9 objective metrics** to measure autonomous deployability:

| Category | Metrics | Current Results |
|----------|---------|----------------|
| **Core Functionality** | Build, Runtime, Type Safety, Tests | 90%, 90%, 0%, 0% |
| **Databricks Integration** | DB Connectivity, Data Returned | 90%, 0% |
| **UI** | UI Renders | 0% |
| **Agentic DevX** | Local Runability, Deployability | 3.0/5, 3.0/5 |

**See [eval-docs/evals.md](eval-docs/evals.md) for complete metric definitions.**

### MLflow Integration

**Track evaluation quality over time** using Databricks Managed MLflow:

- ğŸ“Š **Automatic Tracking**: Every evaluation run logged to MLflow
- ğŸ“ˆ **Metrics Trends**: Monitor success rates, quality scores, cost efficiency
- ğŸ” **Run Comparison**: Compare MCP vs Vanilla modes, track improvements
- ğŸ“¦ **Artifacts**: All reports automatically saved and versioned

```bash
# Automatic MLflow tracking with run scripts
./run_vanilla_eval.sh  # Tracked as "vanilla_sdk" mode
./run_mcp_eval.sh      # Tracked as "mcp" mode

# Compare recent runs
uv run cli/mlflow_compare.py

# View in Databricks UI
# Navigate to ML â†’ Experiments â†’ /Shared/klaudbiusz-evaluations
```

**See [MLFLOW_INTEGRATION.md](MLFLOW_INTEGRATION.md) for detailed MLflow documentation.**

### Key Innovation: Agentic Evaluation

We use an **AI agent to evaluate apps** using objective, measurable criteria:

- **Agent-Driven:** Agent reads files, discovers commands, executes builds/tests
- **Stack Agnostic:** Works with TypeScript, Python, Streamlit, any framework
- **Zero Hardcoding:** No assumptions about app structure or build process
- **Reproducible:** Same app â†’ same evaluation results

**See [eval-docs/EVALUATION_METHODOLOGY.md](eval-docs/EVALUATION_METHODOLOGY.md) for detailed agentic evaluation approach.**

## Documentation

### Framework & Methodology
- **[eval-docs/evals.md](eval-docs/evals.md)** - Complete 9-metric framework definition
- **[eval-docs/EVALUATION_METHODOLOGY.md](eval-docs/EVALUATION_METHODOLOGY.md)** - Zero-bias evaluation methodology
- **[eval-docs/DORA_METRICS.md](eval-docs/DORA_METRICS.md)** - DORA metrics integration & agentic DevX
- **[eval-docs/LLM_BASED_EVALUATION.md](eval-docs/LLM_BASED_EVALUATION.md)** - LLM-based stack-agnostic evaluation

### Integration & Tracking
- **[MLFLOW_INTEGRATION.md](MLFLOW_INTEGRATION.md)** - MLflow tracking for evaluation runs, metrics, and trends

### Generation Modes
- **[VANILLA_SDK_MODE.md](VANILLA_SDK_MODE.md)** - Pure Claude SDK mode (63% cheaper, no MCP tools)

### Results (Generated by Evaluation)
- **EVALUATION_REPORT.md** - Latest evaluation results (root level)
- **evaluation_report.json** - Structured data (root level)
- **evaluation_report.csv** - Spreadsheet format (root level)

### Archives
- **klaudbiusz_evaluation_*.tar.gz** - Historical evaluation archives
- **App.build Evals 2.0.docx** - Executive summary

## Project Structure

```
klaudbiusz/
â”œâ”€â”€ README.md                        # This file
â”œâ”€â”€ run_all_evals.sh                 # Run both modes with comparison
â”œâ”€â”€ run_vanilla_eval.sh              # Full pipeline: Vanilla SDK mode
â”œâ”€â”€ run_mcp_eval.sh                  # Full pipeline: MCP mode
â”œâ”€â”€ eval-docs/                       # Evaluation framework docs
â”‚   â”œâ”€â”€ evals.md                    # 9-metric definitions
â”‚   â”œâ”€â”€ EVALUATION_METHODOLOGY.md   # Zero-bias methodology
â”‚   â””â”€â”€ DORA_METRICS.md             # DORA & agentic DevX
â”œâ”€â”€ cli/                             # Generation & evaluation scripts
â”‚   â”œâ”€â”€ bulk_run.py                 # Batch app generation
â”‚   â”œâ”€â”€ evaluate_all_agent.py       # Agentic evaluation (~45 lines)
â”‚   â”œâ”€â”€ archive_evaluation.sh       # Create evaluation archive
â”‚   â””â”€â”€ cleanup_evaluation.sh       # Clean generated apps
â”œâ”€â”€ results_<timestamp>/             # Comparison results (gitignored)
â”‚   â”œâ”€â”€ vanilla/                    # Vanilla SDK results
â”‚   â”œâ”€â”€ mcp/                        # MCP results
â”‚   â”œâ”€â”€ combined_metadata.json      # Combined run details
â”‚   â””â”€â”€ COMPARISON.md               # Side-by-side comparison
â”œâ”€â”€ results_latest/                  # Symlink to latest results
â””â”€â”€ archive/                         # Historical evaluation archives
```

## Workflows

### Recommended: Complete Comparison

```bash
# Set environment variables
export DATABRICKS_HOST=https://...
export DATABRICKS_TOKEN=dapi...
export ANTHROPIC_API_KEY=sk-ant-...

# Run BOTH modes for side-by-side comparison
./run_all_evals.sh
```

**Outputs** (`results_<timestamp>/`):
```
results_20251022_143052/
â”œâ”€â”€ vanilla/
â”‚   â”œâ”€â”€ evaluation_report.json
â”‚   â”œâ”€â”€ EVALUATION_REPORT.md
â”‚   â”œâ”€â”€ run_metadata.json
â”‚   â””â”€â”€ app/  (20 Streamlit apps)
â”œâ”€â”€ mcp/
â”‚   â”œâ”€â”€ evaluation_report.json
â”‚   â”œâ”€â”€ EVALUATION_REPORT.md
â”‚   â”œâ”€â”€ run_metadata.json
â”‚   â””â”€â”€ app/  (20 TypeScript/tRPC apps)
â”œâ”€â”€ combined_metadata.json
â””â”€â”€ COMPARISON.md  (timing comparison)
```

**Quick access:** `results_latest/` symlinks to latest run

### Individual Mode Pipelines

```bash
# Run only Vanilla SDK mode
./run_vanilla_eval.sh

# Run only MCP mode
./run_mcp_eval.sh
```

### Manual Workflow

1. Write natural language prompt
2. Generate: `uv run cli/bulk_run.py`
3. Evaluate: `uv run cli/evaluate_all_agent.py`
4. Review: `cat EVALUATION_REPORT.md`
5. Deploy apps that pass checks

### Archive & Clean Workflow

```bash
# Create archive of apps + reports
./cli/archive_evaluation.sh

# Verify checksum
shasum -a 256 -c klaudbiusz_evaluation_*.tar.gz.sha256

# Clean up generated apps
./cli/cleanup_evaluation.sh
```

## Requirements

- Python 3.11+
- uv (Python package manager)
- Docker (for builds and runtime checks)
- Node.js 18+ (for generated apps)
- Databricks workspace with access token

## Environment Variables

You can set environment variables either via shell export or using a `.env` file:

**Option 1: Shell export**
```bash
export DATABRICKS_HOST=https://your-workspace.databricks.com
export DATABRICKS_TOKEN=dapi...
export ANTHROPIC_API_KEY=sk-ant-...

# Optional for logging
export DATABASE_URL=postgresql://...
```

**Option 2: .env file**
Create a `.env` file in the project root:
```bash
# Required for generation and evaluation
DATABRICKS_HOST=https://your-workspace.databricks.com
DATABRICKS_TOKEN=dapi...
ANTHROPIC_API_KEY=sk-ant-...

# Optional for logging
DATABASE_URL=postgresql://...
```

All scripts (`bulk_run.py`, `main.py`, `evaluate_all_agent.py`) automatically load `.env` if present.

## Core Principle

> If an AI agent cannot autonomously deploy its own generated code, that code is not production-ready.

All metrics are **objective, reproducible, and automatable** - no subjective quality assessments.

**See [eval-docs/EVALUATION_METHODOLOGY.md](eval-docs/EVALUATION_METHODOLOGY.md) for our zero-bias philosophy.**

## License

Apache 2.0

---

**Latest Evaluation:** October 17, 2025
**Success Rate:** 90% deployment-ready (18/20 apps)
**Lead Time:** 6-9 minutes (prompt â†’ production-ready code)
