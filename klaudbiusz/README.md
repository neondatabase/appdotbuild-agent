# Klaudbiusz

AI-powered Databricks application generator with objective evaluation framework. A thin wrapper around the Claude Agent SDK leveraging the `edda` MCP.

## Overview

Klaudbiusz generates production-ready Databricks applications from natural language prompts and evaluates them using 9 objective, zero-bias metrics. This enables autonomous deployment workflows where AI-generated code can be automatically validated and deployed without human review.

**Current Results:** 90% of generated apps (18/20) are production-ready and deployable.

## Quick Start

### Setup Environment

Create a `.env` file in the `klaudbiusz/` directory (copy from `.env.example`):

```bash
cd klaudbiusz
cp .env.example .env
# Edit .env with your credentials
```

`.env` file contents:
```bash
DATABRICKS_HOST=https://your-workspace.databricks.com
DATABRICKS_TOKEN=dapi...
ANTHROPIC_API_KEY=sk-ant-...
```

### Generate Applications

```bash
cd klaudbiusz

# Generate a single app (Claude backend, default)
uv run cli/single_run.py "Create a customer churn analysis dashboard"

# Use LiteLLM backend with specific model
uv run cli/single_run.py "Create a customer churn analysis dashboard" \
  --backend=litellm --model=openrouter/minimax/minimax-m2

# Batch generate from prompts (databricks set by default)
uv run cli/bulk_run.py

# Batch generate with test prompts
uv run cli/bulk_run.py --prompts=test

# Batch generate with LiteLLM backend
uv run cli/bulk_run.py --backend=litellm --model=gemini/gemini-2.5-pro
```

### Evaluate Generated Apps

```bash
cd klaudbiusz

# Evaluate all apps
uv run cli/evaluate_all.py

# Partial evaluation (filter apps)
uv run cli/evaluate_all.py --limit 5                    # First 5 apps
uv run cli/evaluate_all.py --apps app1 app2             # Specific apps
uv run cli/evaluate_all.py --pattern "customer*"        # Pattern matching
uv run cli/evaluate_all.py --skip 10 --limit 5          # Skip first 10, evaluate next 5

# Evaluate single app
uv run cli/evaluate_app.py ../app/customer-churn-analysis
```

**Results are automatically logged to MLflow:** Navigate to `ML â†’ Experiments â†’ /Shared/klaudbiusz-evaluations` in Databricks UI.

## Evaluation Framework

We use **9 objective metrics** to measure autonomous deployability:

| Category | Metrics | Current Results |
|----------|---------|----------------|
| **Core Functionality** | Build, Runtime, Type Safety, Tests | 90%, 90%, 0%, 0% |
| **Databricks Integration** | DB Connectivity, Data Returned | 90%, 0% |
| **UI** | UI Renders | 0% |
| **Agentic DevX** | Local Runability, Deployability | 3.0/5, 3.0/5 |

**See [eval-docs/evals.md](eval-docs/evals.md) for complete metric definitions.**

### MLflow Integration

**Track evaluation quality over time** using Databricks Managed MLflow:

- ðŸ“Š **Automatic Tracking**: Every evaluation run logged to MLflow
- ðŸ“ˆ **Metrics Trends**: Monitor success rates, quality scores, cost efficiency
- ðŸ” **Run Comparison**: Compare evaluation runs and track improvements
- ðŸ“¦ **Artifacts**: All reports automatically saved and versioned

### Key Innovation: Agentic DevX

We measure **whether an AI agent can autonomously run and deploy the code** with zero configuration:

- **Local Runability:** Can run with `npm install && npm start`? (3.0/5)
- **Deployability:** Can deploy with `docker build && docker run`? (3.0/5)

**See [eval-docs/DORA_METRICS.md](eval-docs/DORA_METRICS.md) for detailed agentic evaluation approach.**

## Documentation

### Framework & Methodology
- **[eval-docs/evals.md](eval-docs/evals.md)** - Complete 9-metric framework definition
- **[eval-docs/EVALUATION_METHODOLOGY.md](eval-docs/EVALUATION_METHODOLOGY.md)** - Zero-bias evaluation methodology
- **[eval-docs/DORA_METRICS.md](eval-docs/DORA_METRICS.md)** - DORA metrics integration & agentic DevX

### Results (Generated by Evaluation)
- **EVALUATION_REPORT.md** - Latest evaluation results (root level)
- **evaluation_report.json** - Structured data (root level)
- **evaluation_report.csv** - Spreadsheet format (root level)

### Archives
- **klaudbiusz_evaluation_*.tar.gz** - Historical evaluation archives
- **App.build Evals 2.0.docx** - Executive summary

## Project Structure

```
klaudbiusz/
â”œâ”€â”€ README.md                        # This file
â”œâ”€â”€ eval-docs/                       # Evaluation framework docs
â”‚   â”œâ”€â”€ evals.md                    # 9-metric definitions
â”‚   â”œâ”€â”€ EVALUATION_METHODOLOGY.md   # Zero-bias methodology
â”‚   â””â”€â”€ DORA_METRICS.md             # DORA & agentic DevX
â”œâ”€â”€ app/                             # Generated applications (gitignored)
â”œâ”€â”€ cli/                             # Generation & evaluation scripts
â”‚   â”œâ”€â”€ single_run.py               # Single app generation
â”‚   â”œâ”€â”€ bulk_run.py                 # Batch app generation
â”‚   â”œâ”€â”€ analyze_trajectories.py     # Get LLM recommendations based on previous runs
â”‚   â”œâ”€â”€ evaluate_all.py             # Batch evaluation
â”‚   â”œâ”€â”€ evaluate_app.py             # Single app evaluation
â”‚   â”œâ”€â”€ archive_evaluation.sh       # Create evaluation archive
â”‚   â””â”€â”€ cleanup_evaluation.sh       # Clean generated apps
â”œâ”€â”€ EVALUATION_REPORT.md            # Latest results (gitignored)
â”œâ”€â”€ evaluation_report.json          # Latest data (gitignored)
â”œâ”€â”€ evaluation_report.csv           # Latest spreadsheet (gitignored)
â””â”€â”€ klaudbiusz_evaluation_*.tar.gz  # Archives
```

## Workflows

### Development Workflow

1. Write natural language prompt
2. Generate: `uv run cli/single_run.py "your prompt"` or `uv run cli/bulk_run.py`
3. Evaluate: `uv run cli/evaluate_all.py`
4. Review: `cat EVALUATION_REPORT.md`
5. Deploy apps that pass checks

### AI Assisted Edda Improvement Workflow

1. Generate many apps with `uv run cli/bulk_run.py`
2. Analyze the trajectories with `uv run cli/analyze_trajectories.py`
3. Based on the report, improve Edda tools and scaffolding
4. Rerun the evaluation to measure impact

### Archive & Clean Workflow

```bash
# Create archive of apps + reports
./cli/archive_evaluation.sh

# Verify checksum
shasum -a 256 -c klaudbiusz_evaluation_*.tar.gz.sha256

# Clean up generated apps
./cli/cleanup_evaluation.sh
```

## Requirements

- Python 3.11+
- uv (Python package manager)
- Docker (for builds and runtime checks)
- Node.js 18+ (for generated apps)
- Databricks workspace with access token

## Environment Variables

**Recommended:** Use a `.env` file in the `klaudbiusz/` directory:

```bash
# Required for generation and MLflow tracking
DATABRICKS_HOST=https://your-workspace.databricks.com
DATABRICKS_TOKEN=dapi...
ANTHROPIC_API_KEY=sk-ant-...
MLFLOW_EXPERIMENT_NAME=/Shared/klaudbiusz-evaluations

# Optional for logging
DATABASE_URL=postgresql://...
```

All scripts automatically load `.env` if present. Copy `.env.example` to get started:
```bash
cp .env.example .env
```

Alternatively, you can export environment variables manually:
```bash
export DATABRICKS_HOST=https://your-workspace.databricks.com
export DATABRICKS_TOKEN=dapi...
export ANTHROPIC_API_KEY=sk-ant-...
```

## Core Principle

> If an AI agent cannot autonomously deploy its own generated code, that code is not production-ready.

All metrics are **objective, reproducible, and automatable** - no subjective quality assessments.

**See [eval-docs/EVALUATION_METHODOLOGY.md](eval-docs/EVALUATION_METHODOLOGY.md) for our zero-bias philosophy.**

## License

Apache 2.0

---

**Latest Evaluation:** October 17, 2025
**Success Rate:** 90% deployment-ready (18/20 apps)
**Lead Time:** 6-9 minutes (prompt â†’ production-ready code)
